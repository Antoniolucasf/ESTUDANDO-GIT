{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0JG8o9M/AbR0NyOlx8qYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antoniolucasf/ESTUDANDO-GIT/blob/master/script_crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dueP-dmoE3WT",
        "outputId": "14129822-3b7b-43e6-a179-d8cb8f9228ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybboxes==0.1.6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFxXiP3qFZlu",
        "outputId": "75cc2725-25ee-4c25-ffca-605796b3d3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybboxes==0.1.6\n",
            "  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pybboxes==0.1.6) (2.0.2)\n",
            "Downloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pybboxes\n",
            "Successfully installed pybboxes-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCAipEyfFb0X",
        "outputId": "4a1656c0-21b9-4e13-d9db-a0c01c9390e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.9.0+cpu\n",
            "Uninstalling torch-2.9.0+cpu:\n",
            "  Successfully uninstalled torch-2.9.0+cpu\n",
            "Found existing installation: torchvision 0.24.0+cpu\n",
            "Uninstalling torchvision-0.24.0+cpu:\n",
            "  Successfully uninstalled torchvision-0.24.0+cpu\n",
            "Found existing installation: torchaudio 2.9.0+cpu\n",
            "Uninstalling torchaudio-2.9.0+cpu:\n",
            "  Successfully uninstalled torchaudio-2.9.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==1.4.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5ioxm8xFdiK",
        "outputId": "4f0e07ba-af57-4b21-9766-ab3d7e3fb0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.4.0\n",
            "  Downloading albumentations-1.4.0-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.0) (1.16.3)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.0) (0.25.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.0) (6.0.3)\n",
            "Collecting qudida>=0.0.4 (from albumentations==1.4.0)\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: opencv-python>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.0) (4.12.0.88)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from qudida>=0.0.4->albumentations==1.4.0) (1.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qudida>=0.0.4->albumentations==1.4.0) (4.15.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from qudida>=0.0.4->albumentations==1.4.0) (4.12.0.88)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.0) (3.6.1)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.0) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.0) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.0) (2025.12.12)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.0) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.0) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.4.0) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.4.0) (3.6.0)\n",
            "Downloading albumentations-1.4.0-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Installing collected packages: qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.8\n",
            "    Uninstalling albumentations-2.0.8:\n",
            "      Successfully uninstalled albumentations-2.0.8\n",
            "Successfully installed albumentations-1.4.0 qudida-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cv2.dnn import ClassificationModel\n",
        "import cv2\n",
        "import os\n",
        "import pybboxes as pbx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import patches\n",
        "import albumentations as A\n",
        "import json\n",
        "\n",
        "COCO_ANNOT_PATH = \"/content/drive/MyDrive/density 3.0.v1i.coco/train/_annotations.coco.json\"\n",
        "\n",
        "with open(COCO_ANNOT_PATH, \"r\") as f:\n",
        "    COCO_DATASET = json.load(f)\n",
        "\n",
        "def busca_anotacao(nomeImg: str, dataset):\n",
        "    image_id = None\n",
        "    for img in dataset['images']:\n",
        "        if img['file_name'] == nomeImg:\n",
        "            image_id = img['id']\n",
        "            break\n",
        "\n",
        "    if image_id is None:\n",
        "        return [], []\n",
        "\n",
        "    bboxes = []\n",
        "    classes = []\n",
        "\n",
        "    for ann in dataset['annotations']:\n",
        "        if ann['image_id'] == image_id:\n",
        "            bboxes.append(ann['bbox'])\n",
        "            classes.append(ann['category_id'])\n",
        "\n",
        "    return bboxes, classes\n",
        "\n",
        "def calculate_slice_bboxes(\n",
        "    image_height: int,\n",
        "    image_width: int,\n",
        "    slice_height: int = 600,\n",
        "    slice_width: int = 600,\n",
        "    overlap_height_ratio: float = 0.2,\n",
        "    overlap_width_ratio: float = 0.2,\n",
        "):\n",
        "    slice_bboxes = []\n",
        "\n",
        "    y_step = int(slice_height * (1 - overlap_height_ratio))\n",
        "    x_step = int(slice_width * (1 - overlap_width_ratio))\n",
        "\n",
        "    y_positions = []\n",
        "    x_positions = []\n",
        "\n",
        "    # --- Calcular posições Y SEM duplicar ---\n",
        "    y = 0\n",
        "    while True:\n",
        "        y_positions.append(y)\n",
        "        if y + slice_height >= image_height:\n",
        "            break\n",
        "        y = min(y + y_step, image_height - slice_height)\n",
        "\n",
        "    # --- Calcular posições X SEM duplicar ---\n",
        "    x = 0\n",
        "    while True:\n",
        "        x_positions.append(x)\n",
        "        if x + slice_width >= image_width:\n",
        "            break\n",
        "        x = min(x + x_step, image_width - slice_width)\n",
        "\n",
        "    # --- Montar os BBoxes ---\n",
        "    for y_min in y_positions:\n",
        "        for x_min in x_positions:\n",
        "            slice_bboxes.append([\n",
        "                x_min,\n",
        "                y_min,\n",
        "                x_min + slice_width,\n",
        "                y_min + slice_height\n",
        "            ])\n",
        "\n",
        "    return slice_bboxes\n",
        "\n",
        "def voc_to_coco_bbox(x1, y1, x2, y2):\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "    return [float(x1), float(y1), float(w), float(h)]\n",
        "\n",
        "def convert_coco_bbox(\n",
        "    bbox: list,\n",
        "    cls: int,\n",
        "    img_size: tuple,\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Converte bbox do formato COCO para Pascal VOC\n",
        "\n",
        "    COCO: [x_min, y_min, width, height]\n",
        "    VOC:  [xmin, ymin, xmax, ymax]\n",
        "    \"\"\"\n",
        "\n",
        "    dh, dw = img_size\n",
        "    x, y, w, h = bbox\n",
        "\n",
        "    x1 = int(x)\n",
        "    y1 = int(y)\n",
        "    x2 = int(x + w)\n",
        "    y2 = int(y + h)\n",
        "\n",
        "    # Clamp para manter dentro da imagem\n",
        "    if x1 < 0:\n",
        "        x1 = 0\n",
        "    if x2 > dw - 1:\n",
        "        x2 = dw - 1\n",
        "    if y1 < 0:\n",
        "        y1 = 0\n",
        "    if y2 > dh - 1:\n",
        "        y2 = dh - 1\n",
        "\n",
        "    return (cls, [x1, y1, x2, y2])\n",
        "\n",
        "def carregar_anotacoes(labels, image_size, dataset):\n",
        "    bboxs, clss = [], []\n",
        "\n",
        "    bboxes, classes = busca_anotacao(labels, dataset)\n",
        "\n",
        "    for bbox, cls in zip(bboxes, classes):\n",
        "        cls, box_voc = convert_coco_bbox(\n",
        "            bbox=bbox,\n",
        "            cls=cls,\n",
        "            img_size=image_size\n",
        "        )\n",
        "        bboxs.append(box_voc)\n",
        "        clss.append(cls)\n",
        "\n",
        "    return np.array(clss), np.array(bboxs)\n",
        "\n",
        "def crop_dataset(\n",
        "    path_img: str,\n",
        "    size_crop: tuple,\n",
        "    save_dir_img: str,\n",
        "    save_json_path: str\n",
        ") -> int:\n",
        "\n",
        "    os.makedirs(save_dir_img, exist_ok=True)\n",
        "\n",
        "    coco_output = {\n",
        "        \"images\": [],\n",
        "        \"categories\": [\n",
        "            {\n",
        "                \"id\": 1,\n",
        "                \"name\": \"bacilo\",\n",
        "                \"supercategory\": \"bacillus\"\n",
        "            }\n",
        "        ],\n",
        "        \"annotations\": [],\n",
        "    }\n",
        "\n",
        "    CATEGORY_MAP = {7: 1}  # normalização\n",
        "\n",
        "    img_list = sorted(os.listdir(path_img))\n",
        "    split_height, split_width = size_crop\n",
        "\n",
        "    image_id = 0\n",
        "    annotation_id = 1\n",
        "\n",
        "    for img_name in img_list:\n",
        "\n",
        "        img_path = os.path.join(path_img, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        img_h, img_w, _ = image.shape\n",
        "\n",
        "        clss, bbox_lb = carregar_anotacoes(\n",
        "            img_name,\n",
        "            image.shape[:2],\n",
        "            COCO_DATASET\n",
        "        )\n",
        "\n",
        "        if len(bbox_lb) == 0:\n",
        "            continue\n",
        "\n",
        "        slices = calculate_slice_bboxes(\n",
        "            img_h, img_w, split_height, split_width\n",
        "        )\n",
        "\n",
        "        for slice_box in slices:\n",
        "\n",
        "            crop_transform = A.Compose(\n",
        "                [A.Crop(*slice_box)],\n",
        "                bbox_params=A.BboxParams(\n",
        "                    format=\"pascal_voc\",\n",
        "                    label_fields=[\"labels\"],\n",
        "                    min_visibility=0.3,\n",
        "                    min_area=10\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                cropped = crop_transform(\n",
        "                    image=image,\n",
        "                    bboxes=bbox_lb,\n",
        "                    labels=clss\n",
        "                )\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            if len(cropped[\"bboxes\"]) == 0:\n",
        "                continue\n",
        "\n",
        "            # ----- salvar imagem -----\n",
        "            base_name = os.path.splitext(img_name)[0]\n",
        "            crop_name = f\"split_0_id{image_id}_{base_name}.jpg\"\n",
        "            crop_path = os.path.join(save_dir_img, crop_name)\n",
        "\n",
        "            cv2.imwrite(crop_path, cropped[\"image\"])\n",
        "\n",
        "            h_crop, w_crop, _ = cropped[\"image\"].shape\n",
        "\n",
        "            coco_output[\"images\"].append({\n",
        "                \"file_name\": crop_name,\n",
        "                \"height\": h_crop,\n",
        "                \"width\": w_crop,\n",
        "                \"id\": image_id\n",
        "            })\n",
        "\n",
        "            # ----- salvar annotations -----\n",
        "            for cls, (x1, y1, x2, y2) in zip(\n",
        "                cropped[\"labels\"], cropped[\"bboxes\"]\n",
        "            ):\n",
        "                coco_bbox = voc_to_coco_bbox(x1, y1, x2, y2)\n",
        "\n",
        "                cls_norm = CATEGORY_MAP.get(int(cls), 1)\n",
        "\n",
        "                coco_output[\"annotations\"].append({\n",
        "                    \"id\": annotation_id,\n",
        "                    \"image_id\": image_id,\n",
        "                    \"bbox\": coco_bbox,\n",
        "                    \"area\": coco_bbox[2] * coco_bbox[3],\n",
        "                    \"iscrowd\": 0,\n",
        "                    \"category_id\": cls_norm,\n",
        "                    \"segmentation\": []\n",
        "                })\n",
        "\n",
        "                annotation_id += 1\n",
        "\n",
        "            image_id += 1\n",
        "\n",
        "    # ----- salvar JSON -----\n",
        "    with open(save_json_path, \"w\") as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    print(f\"JSON COCO salvo em: {save_json_path}\")\n",
        "    return image_id\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    path_img = \"/content/drive/MyDrive/density 3.0.v1i.coco/train\"\n",
        "    box_size = (600, 600)\n",
        "\n",
        "    dir_img = \"/content/drive/MyDrive/BASE DE DADOS/TREINO/imagens_treino/\"\n",
        "\n",
        "    total = crop_dataset(\n",
        "        path_img=path_img,\n",
        "        size_crop=box_size,\n",
        "        save_dir_img=dir_img,\n",
        "        save_json_path=\"/content/drive/MyDrive/BASE DE DADOS/TREINO/annotations_treino.json\"\n",
        "    )\n",
        "\n",
        "    print(f\"Total de cortes gerados: {total}\")\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXMBRNQK4ijS",
        "outputId": "e9f9e0a1-ec1e-44b9-b61c-d324d5b09191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON COCO salvo em: /content/drive/MyDrive/BASE DE DADOS/TREINO/annotations_treino.json\n",
            "Total de cortes gerados: 7960\n"
          ]
        }
      ]
    }
  ]
}